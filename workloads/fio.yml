---
#
# Runs FIO I/O test on an existing RHCOS cluster
#

- name: Runs FIO I/O on a RHCOS cluster
  hosts: orchestration
  gather_facts: true
  remote_user: "{{orchestration_user}}"
  vars_files:
    - vars/fio.yml
  vars:
    workload_job: "storage-fio"
  tasks:
    - name: Create scale-ci-tooling directory
      file:
        path: "{{ansible_user_dir}}/scale-ci-tooling"
        state: directory

    # Get workload start time
    - name: Get workload start time
      shell: |
        date +%s
      register: starttime_set
      when: enable_prometheus_queries

    - name: Display workload start time from date
      debug:
        msg: "{{ starttime_set }}"
      when: enable_prometheus_queries

    - name: Display workload start time
      debug:
        msg: "{{ ansible_date_time.epoch }}"
      when: enable_prometheus_queries

    - name: Display workload start time - 120 seconds
      debug:
        msg: "{{ ansible_date_time.epoch | int - ( 60 * 2 ) }}"
      when: enable_prometheus_queries

#    - name: Display type #2
#      debug:
#        msg: "{{ type(ansible_date_time.epoch) }}"
#      when: enable_prometheus_queries
#
#    - name: Display type #2
#      debug:
#        msg: "{{ type(starttime_set)  }}"
#      when: enable_prometheus_queries

    # Set workload start time
    - name: Set workload starttime
      shell: |
        oc create configmap -n scale-ci-tooling scale-ci-{{ job_name }}-{{ build_number }}-run-info --from-literal=starttime="{{starttime_set.stdout}}" --dry-run -o yaml | kubectl apply -f -
      when: enable_prometheus_queries

    - name: Copy workload files
      copy:
        src: "{{item.src}}"
        dest: "{{item.dest}}"
      with_items:
        - src: scale-ci-tooling-ns.yml
          dest: "{{ansible_user_dir}}/scale-ci-tooling/scale-ci-tooling-ns.yml"

    - name: Slurp kubeconfig file
      slurp:
        src: "{{kubeconfig_file}}"
      register: kubeconfig_file_slurp

    - name: Slurp ssh private key file
      slurp:
        src: "{{pbench_ssh_private_key_file}}"
      register: pbench_ssh_private_key_file_slurp

    - name: Slurp ssh public key file
      slurp:
        src: "{{pbench_ssh_public_key_file}}"
      register: pbench_ssh_public_key_file_slurp

    - name: Block to set clustername
      block:
        - name: Get cluster name
          shell: |
            {%raw%}oc get clusterversion -o jsonpath="{.items[].spec.clusterID}"{%endraw%}
          register: cluster_name

        - name: Create tooling service account
          set_fact:
            snafu_cluster_name: cluster_name.stdout
          when: cluster_name is succeeded
      when: snafu_cluster_name == ""

    - name: Get the nodes
      shell: |
        oc get nodes -l node-role.kubernetes.io/worker= --no-headers | head -n {{fiotest_maxpods}} | awk '{print $1}'
      register: fiotest_nodes
      when: workload_nodeselector == ""
    - name: show lines
      debug:
        msg: "Lines {{fiotest_nodes}}"
    - name: Create Node List
      set_fact:
        fiotest_nodes_list: "{{fiotest_nodes.stdout_lines|list}}"
      when: workload_nodeselector == ""

    - name: Get the nodes (Override)
      shell: |
        oc get nodes -l node-role.kubernetes.io/worker=,{{workload_nodeselector}} --no-headers | head -n {{fiotest_maxpods}} | awk '{print $1}'
      register: fiotest_nodes
      when: workload_nodeselector != ""
    - name: show lines
      debug:
        msg: "Lines {{fiotest_nodes}}"
    - name: Create Node List (Override)
      set_fact:
        fiotest_nodes_list: "{{fiotest_nodes.stdout_lines|list}}"
      when: workload_nodeselector != ""

    - name: Template workload templates
      template:
        src: "{{item.src}}"
        dest: "{{item.dest}}"
        lstrip_blocks: yes
      with_items:
        - src: pbench-cm.yml.j2
          dest: "{{ansible_user_dir}}/scale-ci-tooling/pbench-cm.yml"
        - src: pbench-ssh-secret.yml.j2
          dest: "{{ansible_user_dir}}/scale-ci-tooling/pbench-ssh-secret.yml"
        - src: kubeconfig-secret.yml.j2
          dest: "{{ansible_user_dir}}/scale-ci-tooling/kubeconfig-secret.yml"
        - src: workload-job.yml.j2
          dest: "{{ansible_user_dir}}/scale-ci-tooling/workload-job.yml"
        - src: workload-fio-script-cm.yml.j2
          dest: "{{ansible_user_dir}}/scale-ci-tooling/workload-fio-script-cm.yml"
        - src: workload-env.yml.j2
          dest: "{{ansible_user_dir}}/scale-ci-tooling/workload-storage-fio-env.yml"

    - name: Check if scale-ci-tooling namespace exists
      shell: |
        oc get projects | grep "scale-ci-tooling"
      ignore_errors: true
      changed_when: false
      register: scale_ci_tooling_ns_exists
    - name: print status of scale_ci_tooling_ns_exists
      debug:
        var: scale_ci_tooling_ns_exists

    - name: Ensure any stale scale-ci-storage-fio job is deleted
      shell: |
        oc delete job scale-ci-storage-fio -n scale-ci-tooling
      register: scale_ci_tooling_project
      failed_when: scale_ci_tooling_project.rc == 0
      until: scale_ci_tooling_project.rc == 1
      retries: 60
      delay: 1
      when: scale_ci_tooling_ns_exists.rc == 0

    - name: Block for non-existing tooling namespace
      block:
        - name: Create tooling namespace
          shell: |
            oc create -f {{ansible_user_dir}}/scale-ci-tooling/scale-ci-tooling-ns.yml
      when: scale_ci_tooling_ns_exists.rc != 0

    - name: Create tooling service account
      shell: |
        oc create serviceaccount useroot -n scale-ci-tooling
        oc adm policy add-scc-to-user privileged -z useroot -n scale-ci-tooling
      when: enable_pbench_agents|bool

    - name: Add serviceaccount for useroot when pbench is not used
      shell: |
        oc delete serviceaccount -n scale-ci-tooling useroot
        oc create serviceaccount useroot -n scale-ci-tooling
      when: not enable_pbench_agents|bool

    - name: Create/replace kubeconfig secret
      shell: |
        oc replace --force -n scale-ci-tooling -f "{{ansible_user_dir}}/scale-ci-tooling/kubeconfig-secret.yml"

    - name: Create/replace the pbench configmap
      shell: |
        oc replace --force -n scale-ci-tooling -f "{{ansible_user_dir}}/scale-ci-tooling/pbench-cm.yml"

    - name: Create/replace pbench ssh secret
      shell: |
        oc replace --force -n scale-ci-tooling -f "{{ansible_user_dir}}/scale-ci-tooling/pbench-ssh-secret.yml"

    - name: Create/replace workload script configmap
      shell: |
        oc replace --force -n scale-ci-tooling -f "{{ansible_user_dir}}/scale-ci-tooling/workload-fio-script-cm.yml"

    - name: Create/replace workload script environment configmap
      shell: |
        oc replace --force -n scale-ci-tooling -f "{{ansible_user_dir}}/scale-ci-tooling/workload-storage-fio-env.yml"

    - name: Enable azure authentication when running ocp on azure
      include_role:
        name: azure-auth
      when: azure_auth and azure_auth_file != ""

    - name: Create/replace workload job to that runs workload script
      shell: |
        oc replace --force -n scale-ci-tooling -f "{{ansible_user_dir}}/scale-ci-tooling/workload-job.yml"

    - name: Poll until job is running
      shell: |
        oc get pods --selector=job-name=scale-ci-storage-fio -n scale-ci-tooling -o json
      register: pod_json
      retries: 60
      delay: 2
      until: pod_json.stdout | from_json | json_query('items[0].status.phase==`Running`')

    - name: Poll until job is complete
      shell: |
        oc get job scale-ci-storage-fio -n scale-ci-tooling -o json
      register: job_json
      retries: "{{job_completion_poll_attempts}}"
      delay: 10
      until: job_json.stdout | from_json | json_query('status.succeeded==`1` || status.failed==`1`')
      failed_when: job_json.stdout | from_json | json_query('status.succeeded==`1`') == false
      when: job_completion_poll_attempts|int > 0

    - name: Set up PromQL queries for workload report
      include_role:
        name: promql_queries
      when: enable_prometheus_queries
